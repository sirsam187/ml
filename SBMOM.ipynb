{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to initialize random weights and biases\n",
    "def initialize_parameters(input_size):\n",
    "    np.random.seed(42)\n",
    "    weights = np.random.randn(input_size, 1)\n",
    "    bias = np.random.randn(1)\n",
    "    return weights, bias\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Forward pass through the neural network\n",
    "def forward(X, weights, bias):\n",
    "    return sigmoid(np.dot(X, weights) + bias)\n",
    "\n",
    "# Calculate squared error loss\n",
    "def calculate_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "# Update parameters using batch momentum-based gradient descent\n",
    "def update_parameters(X, y, weights, bias, learning_rate, momentum, prev_delta_weights, prev_delta_bias):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = forward(X, weights, bias)\n",
    "    \n",
    "    # Compute gradients\n",
    "    error = y_pred - y\n",
    "    delta_weights = (1/m) * np.dot(X.T, error)\n",
    "    delta_bias = (1/m) * np.sum(error)\n",
    "    \n",
    "    # Update weights and bias with momentum\n",
    "    delta_weights = momentum * prev_delta_weights + (1 - momentum) * delta_weights\n",
    "    delta_bias = momentum * prev_delta_bias + (1 - momentum) * delta_bias\n",
    "    \n",
    "    weights -= learning_rate * delta_weights\n",
    "    bias -= learning_rate * delta_bias\n",
    "    \n",
    "    return weights, bias, delta_weights, delta_bias\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X = np.array([[0.5], [2.5]])\n",
    "y = np.array([[0.2], [0.9]])\n",
    "\n",
    "# Initialize parameters\n",
    "weights, bias = initialize_parameters(X.shape[1])\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "epochs = 100\n",
    "\n",
    "# Lists to store loss values for plotting\n",
    "loss_history = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass and calculate loss\n",
    "    y_pred = forward(X, weights, bias)\n",
    "    loss = calculate_loss(y, y_pred)\n",
    "    \n",
    "    # Update parameters\n",
    "    weights, bias, delta_weights, delta_bias = update_parameters(X, y, weights, bias, learning_rate, momentum, 0, 0)\n",
    "    \n",
    "    # Store loss for plotting\n",
    "    loss_history.append(loss)\n",
    "\n",
    "    # Display updated weights and biases\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Loss: {loss:.4f} - Weights: {weights.flatten()} - Bias: {bias[0]}\")\n",
    "\n",
    "# Plot loss w.r.t. epoch values\n",
    "plt.plot(range(1, epochs + 1), loss_history, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs. Epoch')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
